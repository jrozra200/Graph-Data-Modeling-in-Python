{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buidling the Knowledge Graph and performing Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "import csv\n",
    "import scispacy\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a knowledge graph schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./data/20k_abstracts_clean.csv', 'r') as c:\n",
    "\treader = csv.reader(c)\n",
    "\tdata = [line for line in reader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobrozran/opt/anaconda3/envs/graphs/lib/python3.8/site-packages/spacy/language.py:2141: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_sci_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IgE, sensitization, Aspergillus fumigatus, positive sputum, fungal, culture result, patients, refractory asthma, patients, antifungal, treatment, voriconazole, asthma-related, outcomes, patients, asthma, IgE, sensitized, fumigatus, Asthmatic, patients, IgE, sensitized, fumigatus, history, severe, exacerbations, months, treated, months, voriconazole, observation, months, double-blind, placebo-controlled, randomized design, Primary outcomes, improvement, quality of life, treatment, reduction, severe, exacerbations, months, study, Sixty-five, patients, randomized, patients, started, treatment, voriconazole, placebo, intention-to-treat analysis, patients, months, medication, voriconazole, placebo, groups, severe, exacerbations, patient, per year, mean difference, CI, quality of life, Asthma Quality of Life Questionnaire, groups, CI, secondary outcome, months, treatment, voriconazole, patients, moderate-to-severe asthma, IgE, sensitized, rate, severe, exacerbations, quality of life, markers, asthma control]\n"
     ]
    }
   ],
   "source": [
    "text = data[0][1]\n",
    "doc = nlp(text)\n",
    "print(list(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_entities = [[row[0], nlp(row[1]).ents] for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', ['ige', 'sensitization', 'aspergillus fumigatus', 'positive sputum', 'fungal', 'culture result', 'patients', 'refractory asthma', 'patients', 'antifungal', 'treatment', 'voriconazole', 'asthma-related', 'outcomes', 'patients', 'asthma', 'ige', 'sensitized', 'fumigatus', 'asthmatic', 'patients', 'ige', 'sensitized', 'fumigatus', 'history', 'severe', 'exacerbations', 'months', 'treated', 'months', 'voriconazole', 'observation', 'months', 'double-blind', 'placebo-controlled', 'randomized design', 'primary outcomes', 'improvement', 'quality of life', 'treatment', 'reduction', 'severe', 'exacerbations', 'months', 'study', 'sixty-five', 'patients', 'randomized', 'patients', 'started', 'treatment', 'voriconazole', 'placebo', 'intention-to-treat analysis', 'patients', 'months', 'medication', 'voriconazole', 'placebo', 'groups', 'severe', 'exacerbations', 'patient', 'per year', 'mean difference', 'ci', 'quality of life', 'asthma quality of life questionnaire', 'groups', 'ci', 'secondary outcome', 'months', 'treatment', 'voriconazole', 'patients', 'moderate-to-severe asthma', 'ige', 'sensitized', 'rate', 'severe', 'exacerbations', 'quality of life', 'markers', 'asthma control']], ['1', ['opioid', 'antagonists', 'naltrexone', 'positive', 'alprazolam', 'attenuate', 'abuse-related', 'effects', 'stimulants', 'amphetamine', 'higher', 'doses', 'efficacy', 'side effects', 'naltrexone', 'alprazolam', 'efficacy', 'untoward effects', 'constituent', 'compounds', 'pilot study', 'hypothesis', 'acute pretreatment', 'combination', 'naltrexone', 'alprazolam', 'clinically', 'physiological effects', 'negative', 'subjective effects', 'positive', 'subjective effects', 'd-amphetamine', 'constituent', 'drugs', 'nontreatment-seeking', 'stimulant-using', 'individuals', 'outpatient', 'experiment', 'oral d-amphetamine', 'administered', 'acute pretreatment', 'naltrexone', 'alprazolam', 'subjective effects', 'psychomotor task', 'physiological', 'oral d-amphetamine', 'prototypical', 'physiological', 'stimulant-like positive', 'vas', 'active/alert/energetic', 'good effect', 'high', 'pretreatment', 'naltrexone', 'alprazolam', 'combination', 'clinically', 'negative', 'subjective effects', 'naltrexone', 'alprazolam', 'attenuated', 'subjective effects', 'd-amphetamine', 'combination', 'attenuated', 'subjective effects', 'constituent', 'drugs', 'results', 'evaluation', 'opioid receptor', 'gabaa-positive modulator', 'clinically', 'experimental conditions', 'effect', 'chronic dosing', 'drugs', 'methamphetamine', 'self-administration']], ['2', ['sequencing', 'learning materials', 'influences', 'knowledge', 'learners construct', 'learning theorists', 'sequencing', 'instruction', 'solving', 'problems', 'consensus', 'explicit', 'instruction', 'instruction', 'unclear', 'tested', 'impact', 'conceptual instruction', 'mathematics', 'conceptual instruction', 'delayed', 'learning processes', 'theories', 'learning', 'second', 'third-grade', 'children', 'randomized experiment', 'children', 'instruction', 'concept', 'math', 'solve', 'equivalence', 'problems', 'feedback', 'conceptual instruction', 'procedural', 'conceptual knowledge', 'equation structures', 'delaying', 'instruction', 'problem solving', 'conceptual instruction', 'problem solving', 'increasing', 'quality', 'explanations', 'attempted procedures', 'conceptual instruction', 'problem solving', 'effective', 'sequencing', 'activities', 'reverse', 'results', 'findings', 'instruction', 'delayed']], ['3', ['patient', 'adherence', 'appointments', 'improving', 'outcomes', 'health care', 'no-show', 'appointments', 'suboptimal', 'resource use', 'patient', 'telephone reminders', 'cancer care', 'adherence', 'disadvantaged', 'populations', 'cost-effective', 'patients', 'clinics', 'academic cancer', 'patients', 'miss', 'appointment', 'clinic', 'no-shows', 'identified', 'predictive model', 'randomized', 'intervention', 'usual-care group', 'intervention group', 'telephone', 'bilingual', 'patient', 'days', 'day', 'appointment', '5-month', 'period', 'appointments', 'scheduled', 'patient appointments', 'risk', 'event', 'patient', 'no-show', 'intervention group', 'control group', 'patient', 'family member', 'associated with', 'no-show rate', 'compared', 'leaving', 'message', 'no contact', 'no-show rate', 'telephone navigation', 'patients', 'risk', 'visit', 'nonadherence', 'effectively', 'patient', 'adherence', 'cancer clinic', 'appointments', 'studies', 'long-term', 'impact', 'patient', 'outcomes', 'short-term', 'gains', 'optimization', 'resources']], ['4', ['insufficient skills', 'drug dose', 'increase', 'risk', 'medication errors', 'nurses', 'struggle', 'calculations', 'learning flexibility', 'cost', 'e-learning', 'alternative', 'classroom teaching', 'study', 'learning outcome', 'risk', 'error', 'course', 'drug dose', 'calculations', 'nurses', 'methods', 'randomised controlled open study', 'nurses', 'hospitals', 'primary healthcare', 'randomised', 'e-learning', 'classroom teaching', '2-day course', 'nurses', 'multiple choice test', 'drug dose', 'calculations', 'tasks', 'alternative answers', 'score', 'statement', 'certainty', 'answer', 'score', 'high risk', 'error', 'answer', 'results', 'sd', 'men', 'women', 'participated', 'study', 'aged', 'years', 'answers', 'e-learning', 'classroom teaching', 'ns', 'improvement', 'ns', 'classroom learning', 'e-learning', 'participants', 'pretest', 'score', 'e-learning', 'evaluation', 'specific value', 'working situation', 'risk', 'error', 'groups', 'course', 'study', 'learning outcome', 'risk', 'error', 'e-learning', 'classroom teaching', 'drug dose', 'calculations', 'learning outcome', 'weak', 'precourse knowledge', 'associated with', 'outcome', 'classroom teaching']]]\n"
     ]
    }
   ],
   "source": [
    "abstract_entities = [[row[0], [str(ent).lower() for ent in row[1]]] for row in abstract_entities]\n",
    "print(abstract_entities[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = [row[1] for row in abstract_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "entities = itertools.chain.from_iterable(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "entity_freq = dict(Counter(entities))\n",
    "entity_freq = dict(sorted(entity_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "print(entity_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_freq = {ent: value for ent, value in entity_freq.items() if value > 100}\n",
    "print(len(high_freq))\n",
    "print(len(entity_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq = {ent: value for ent, value in entity_freq.items() if value == 1}\n",
    "print(len(low_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_terms = [ent for ent, value in entity_freq.items() if value > 100 or value == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_entities = [[row[0], [ent for ent in row[1] if ent not in removed_terms]]\n",
    "                  \tfor row in abstract_entities]\n",
    "print(abstract_entities[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [abstract[1] for abstract in abstract_entities]  # get just ents no abstract IDs\n",
    "unique_terms = list(set(itertools.chain.from_iterable(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_ids = {term: i for i, term in enumerate(unique_terms, len(data))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(term_ids['ige'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = []\n",
    "for abstract_id, terms in abstract_entities:\n",
    "    term_freq = dict(Counter(terms))\n",
    "    for term, freq in term_freq.items():\n",
    "        edgelist.append([int(term_ids[term]), int(abstract_id), freq])\n",
    " \n",
    "print(edgelist[:10])\n",
    "assert [term_ids['ige'], 0, 4] in edgelist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = igraph.Graph(directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_vertices(len(term_ids) + len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [abstract[1] for abstract in data] + [ent for ent, _ in term_ids.items()]\n",
    "g.vs['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.vs[term_ids['ige']]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['abstract' for _ in data] + ['term' for _ in term_ids.items()]\n",
    "g.vs['type'] = types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.vs[term_ids['ige']]['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [[source, target] for source, target, _ in edgelist]\n",
    "frequencies = [freq for _, _, freq in edgelist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_edges(edges)\n",
    "g.es['frequency'] = frequencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge graph analysis and community detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examing knowledge graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(g.vs))\n",
    "print(len(g.es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_components = g.clusters(mode='weak')\n",
    "print(connected_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_nodes = g.vs.select(type_eq='abstract')\n",
    "term_nodes = g.vs.select(type_eq='term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_degree = g.degree(abstract_nodes)\n",
    "term_degree = g.degree(term_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(abstract_degree, bins=20, edgecolor='black')\n",
    "plt.xlabel('Abstract Node Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('charts/abstract_degree.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(term_degree, bins=20, edgecolor='black')\n",
    "plt.xlabel('Term Node Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('charts/term_node_degree.jpg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying abstracts of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoga_node_id = g.vs.select(text_eq='yoga')[0].index\n",
    "yoga_abstract_nodes = g.neighbors(g.vs[yoga_node_id])\n",
    "yoga_abstracts = [g.vs[neighbor]['text'] for neighbor in yoga_abstract_nodes]\n",
    "print(yoga_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_term_nodes = [g.neighbors(node) for node in yoga_abstract_nodes]\n",
    "import itertools\n",
    "related_term_nodes = set(itertools.chain.from_iterable(related_term_nodes))\n",
    "related_terms = g.vs(related_term_nodes)['text']\n",
    "print(related_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(related_terms))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying fields with Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_u = g.as_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_membership = g_u.community_multilevel()\n",
    "print(len(community_membership))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, community in enumerate(community_membership):\n",
    "\tsize = len(community)\n",
    "\tprint(f'Community: {i}, size: {size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest_community = sorted(list(community_membership), key=len)[0]\n",
    "print(smallest_community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_nodes = g.vs[smallest_community].select(type_eq='term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_terms = community_nodes['text']\n",
    "print(community_terms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing build_know_graph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_know_graph.py\n",
    "\"\"\" \n",
    "Name:       build_know_graph.py.py\n",
    "Author(s):  Gary Hutson & Matt Jackson on behalf of Packt publishing\n",
    "Date:       03/02/2022\n",
    "Usage:      build_know_graph.py\n",
    "\"\"\"\n",
    "import igraph\n",
    "import csv\n",
    "import scispacy\n",
    "import spacy\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load in data\n",
    "with open('./data/20k_abstracts_clean.csv', 'r') as c:\n",
    "\treader = csv.reader(c)\n",
    "\tdata = [line for line in reader]\n",
    "\n",
    "# Load in spacy NLP library\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "text = data[0][1]\n",
    "doc = nlp(text)\n",
    "print(list(doc.ents))\n",
    "\n",
    "# Get abstract entities \n",
    "abstract_entities = [[row[0], nlp(row[1]).ents] for row in data]\n",
    "abstract_entities = [[row[0], [str(ent).lower() for ent in row[1]]] for row in abstract_entities]\n",
    "print(abstract_entities[:5])\n",
    "\n",
    "# Get all entities\n",
    "all_entities = [row[1] for row in abstract_entities]\n",
    "\n",
    "# Use itertools to get all entities\n",
    "entities = itertools.chain.from_iterable(all_entities)\n",
    "\n",
    "# Get frequency counts using Counter()\n",
    "entity_freq = dict(Counter(entities))\n",
    "entity_freq = dict(sorted(entity_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "print(entity_freq)\n",
    "\n",
    "high_freq = {ent: value for ent, value in entity_freq.items() if value > 100}\n",
    "print(len(high_freq))\n",
    "print(len(entity_freq))\n",
    "\n",
    "low_freq = {ent: value for ent, value in entity_freq.items() if value == 1}\n",
    "print(len(low_freq))\n",
    "\n",
    "# Remove some terms\n",
    "removed_terms = [ent for ent, value in entity_freq.items() if value > 100 or value == 1]\n",
    "abstract_entities = [[row[0], [ent for ent in row[1] if ent not in removed_terms]]\n",
    "                  \tfor row in abstract_entities]\n",
    "print(abstract_entities[0])\n",
    "\n",
    "# Constructing the knowledge graph\n",
    "terms = [abstract[1] for abstract in abstract_entities]  # get just ents no abstract IDs\n",
    "unique_terms = list(set(itertools.chain.from_iterable(terms)))\n",
    "\n",
    "\n",
    "term_ids = {term: i for i, term in enumerate(unique_terms, len(data))}\n",
    "print(term_ids['ige'])\n",
    "\n",
    "# Create the edgelists\n",
    "edgelist = []\n",
    "for abstract_id, terms in abstract_entities:\n",
    "    term_freq = dict(Counter(terms))\n",
    "    for term, freq in term_freq.items():\n",
    "        edgelist.append([int(term_ids[term]), int(abstract_id), freq])\n",
    " \n",
    "print(edgelist[:10])\n",
    "assert [term_ids['ige'], 0, 4] in edgelist\n",
    "\n",
    "# Instantiate igraph as a directed graph\n",
    "g = igraph.Graph(directed=True)\n",
    "# Add verticies\n",
    "g.add_vertices(len(term_ids) + len(data))\n",
    "# Extract text\n",
    "text = [abstract[1] for abstract in data] + [ent for ent, _ in term_ids.items()]\n",
    "g.vs['text'] = text\n",
    "print(g.vs[term_ids['ige']]['text'])\n",
    "types = ['abstract' for _ in data] + ['term' for _ in term_ids.items()]\n",
    "g.vs['type'] = types\n",
    "print(g.vs[term_ids['ige']]['type'])\n",
    "edges = [[source, target] for source, target, _ in edgelist]\n",
    "frequencies = [freq for _, _, freq in edgelist]\n",
    "g.add_edges(edges)\n",
    "g.es['frequency'] = frequencies\n",
    "# Analyze and apply community detection\n",
    "print(len(g.vs))\n",
    "print(len(g.es))\n",
    "# Find weakly connected components\n",
    "connected_components = g.clusters(mode='weak')\n",
    "print(connected_components)\n",
    "# Get the abstract and term nodes\n",
    "abstract_nodes = g.vs.select(type_eq='abstract')\n",
    "term_nodes = g.vs.select(type_eq='term')\n",
    "# Get the degrees\n",
    "abstract_degree = g.degree(abstract_nodes)\n",
    "term_degree = g.degree(term_nodes)\n",
    "\n",
    "# Do some plotting\n",
    "# Abstract degree first\n",
    "plt.hist(abstract_degree, bins=20, edgecolor='black')\n",
    "plt.xlabel('Abstract Node Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('charts/abstract_degree.jpg')\n",
    "plt.show()\n",
    "\n",
    "# Term degree second\n",
    "plt.hist(term_degree, bins=20, edgecolor='black')\n",
    "plt.xlabel('Term Node Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('charts/term_node_degree.jpg')\n",
    "\n",
    "# Identify abstracts of interest\n",
    "yoga_node_id = g.vs.select(text_eq='yoga')[0].index\n",
    "yoga_abstract_nodes = g.neighbors(g.vs[yoga_node_id])\n",
    "yoga_abstracts = [g.vs[neighbor]['text'] for neighbor in yoga_abstract_nodes]\n",
    "print(yoga_abstracts)\n",
    "\n",
    "related_term_nodes = [g.neighbors(node) for node in yoga_abstract_nodes]\n",
    "import itertools\n",
    "related_term_nodes = set(itertools.chain.from_iterable(related_term_nodes))\n",
    "related_terms = g.vs(related_term_nodes)['text']\n",
    "print(related_terms)\n",
    "print(len(related_terms))\n",
    "\n",
    "# Identifying fields with Community Detection\n",
    "g_u = g.as_undirected()\n",
    "community_membership = g_u.community_multilevel()\n",
    "print(len(community_membership))\n",
    "\n",
    "# Loop through communities and return their sizings\n",
    "for i, community in enumerate(community_membership):\n",
    "\tsize = len(community)\n",
    "\tprint(f'Community: {i}, size: {size}')\n",
    "\n",
    "# Get the smallest community\n",
    "smallest_community = sorted(list(community_membership), key=len)[0]\n",
    "print(smallest_community)\n",
    "\n",
    "# Get the community nodes\n",
    "community_nodes = g.vs[smallest_community].select(type_eq='term')\n",
    "community_terms = community_nodes['text']\n",
    "print(community_terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e962125a6bade716827c7cd07d3b7c9b717838910a5075eb32d8b321480d4bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
